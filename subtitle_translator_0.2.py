import subprocess
import threading
import queue
import requests
import tempfile
import logging
import tkinter as tk
import json
import time
import webrtcvad

import nls
import json
import time
import threading

import nltk
from nltk.tokenize import sent_tokenize

class AliyunLiveRecognizer(nls.NlsSpeechTranscriber):
    def __init__(self, token, appkey):
        super().__init__(
            url="wss://nls-gateway-cn-shanghai.aliyuncs.com/ws/v1",
            token=token,
            appkey=appkey,
            on_result_changed=self.on_result_changed,
            on_completed=self.on_completed,
            on_error=self.on_error,
            on_close=self.on_close
        )
        self.running = False
        self.result_map = {}
        self.lock = threading.Lock()
        self.last_index = -1

    def start_stream(self):
        self.result_map = {}
        self.running = True
        threading.Thread(target=self._feed_audio, daemon=True).start()

        self.start(
            aformat='pcm',
            enable_intermediate_result=True,
            enable_punctuation_prediction=True,
            enable_inverse_text_normalization=True
        )

    def _feed_audio(self):
        while self.running:
            try:
                chunk = AUDIO_QUEUE.get()
                for i in range(0, len(chunk), 640):
                    frame = chunk[i:i+640]
                    if len(frame) == 640:
                        self.send_audio(frame)
                        time.sleep(0.01)
            except Exception:
                break

    def stop_stream(self):
        self.running = False
        try:
            self.stop()
        except Exception:
            pass

    def on_result_changed(self, message, *args):
        try:
            msg = json.loads(message)
            payload = msg.get("payload", {})
            text = payload.get("result", "").strip()

            if not hasattr(self, "current_buffer"):
                self.current_buffer = ""

            # æ‹¼æ¥è¯†åˆ«æ–‡æœ¬
            self.current_buffer += " " + text
            self.current_buffer = self.current_buffer.strip()

            # ä½¿ç”¨ nltk è‡ªåŠ¨æ–­å¥
            sentences = sent_tokenize(self.current_buffer)

            if len(sentences) >= 2:
                # âœ… æ­£å¸¸æ–­å¥ï¼Œå¤„ç†å‰ N-1 å¥
                for s in sentences[:-1]:
                    SUBTITLE_QUEUE.put({
                        "text": f"åŸæ–‡: {s.strip()}",
                        "audio": "green",
                        "whisper": "green",
                        "translate": "red",
                        "need_translate": True
                    })
                self.current_buffer = sentences[-1].strip()

            elif len(sentences) == 1:
                # âœ… å¦‚æœåªæœ‰ä¸€å¥ï¼Œä½†å¤ªé•¿ï¼Œä¹Ÿå¼ºåˆ¶æ–­å¥
                if len(sentences[0]) >= 45:
                    sentence = self.current_buffer
                    self.current_buffer = ""

                    SUBTITLE_QUEUE.put({
                        "text": f"åŸæ–‡: {sentence}",
                        "audio": "green",
                        "whisper": "green",
                        "translate": "red",
                        "need_translate": True
                    })

            # âœ… å®æ—¶æ˜¾ç¤ºè¿˜æ²¡æ–­å®Œçš„éƒ¨åˆ†
            if self.current_buffer:
                SUBTITLE_QUEUE.put({
                    "text": f"åŸæ–‡: {self.current_buffer}",
                    "audio": "green",
                    "whisper": "green",
                    "translate": "red",
                    "need_translate": False
                })

        except Exception as e:
            print("å­—å¹•æ–­å¥å¤±è´¥:", e)

    def on_completed(self, message, *args):
        print("è¯†åˆ«å®Œæˆ:", message)

    def on_error(self, message, *args):
        print("è¯†åˆ«é”™è¯¯:", message)

    def on_close(self, *args):
        print("è¯†åˆ«è¿æ¥å…³é—­")

API_KEY = ""
AUDIO_QUEUE = queue.Queue()
SUBTITLE_QUEUE = queue.Queue()

FRAME_DURATION = 30  # ms
FRAME_SIZE = int(16000 * 2 * FRAME_DURATION / 1000)
SILENCE_LIMIT_FRAMES = int(500 / FRAME_DURATION)

vad = webrtcvad.Vad(2)

class SpeechDetector:
    def __init__(self):
        self.buffer = bytearray()
        self.silence_counter = 0

    def process_frame(self, frame):
        if vad.is_speech(frame, 16000):
            self.buffer.extend(frame)
            self.silence_counter = 0
            return None
        else:
            if self.buffer:
                self.silence_counter += 1
                if self.silence_counter >= SILENCE_LIMIT_FRAMES:
                    result = bytes(self.buffer)
                    self.buffer.clear()
                    return result
        return None

def start_gui():
    def gui():
        root = tk.Tk()
        root.title("å®æ—¶å­—å¹•")
        root.attributes("-topmost", True)
        root.attributes("-alpha", 0.92)
        root.configure(bg="black")
        screen_width = root.winfo_screenwidth()
        screen_height = root.winfo_screenheight()
        root.geometry(f"900x280+{(screen_width - 900) // 2}+{screen_height - 350}")

        # çŠ¶æ€ç¯åŒºåŸŸ
        status_frame = tk.Frame(root, bg="black")
        status_frame.pack(anchor="nw", padx=10, pady=5)

        audio_light = tk.Canvas(status_frame, width=20, height=20, bg="black", highlightthickness=0)
        audio_light.create_oval(2, 2, 18, 18, fill="red", tags="light_audio")
        audio_light.pack(side="left", padx=5)

        whisper_light = tk.Canvas(status_frame, width=20, height=20, bg="black", highlightthickness=0)
        whisper_light.create_oval(2, 2, 18, 18, fill="red", tags="light_whisper")
        whisper_light.pack(side="left", padx=5)

        translate_light = tk.Canvas(status_frame, width=20, height=20, bg="black", highlightthickness=0)
        translate_light.create_oval(2, 2, 18, 18, fill="red", tags="light_translate")
        translate_light.pack(side="left", padx=5)

        # å†å²å­—å¹•åŒºï¼ˆå¯æ»šåŠ¨ï¼‰
        history_frame = tk.Frame(root, bg="black")
        history_frame.pack(fill="both", expand=True, padx=10)

        scrollbar = tk.Scrollbar(history_frame)
        scrollbar.pack(side="right", fill="y")

        history_text = tk.Text(history_frame, wrap="word", yscrollcommand=scrollbar.set,
                               font=("Microsoft YaHei", 13), bg="black", fg="white", height=6)
        history_text.pack(side="left", fill="both", expand=True)
        history_text.config(state="disabled")
        scrollbar.config(command=history_text.yview)

        # å®æ—¶å­—å¹•åŒºï¼ˆå›ºå®šä¸¤è¡Œï¼šåŸæ–‡+è¯‘æ–‡ï¼‰
        current_label = tk.Label(root, text="åŸæ–‡ï¼š", font=("Microsoft YaHei", 16),
                                 fg="#00ffcc", bg="black", anchor="w", wraplength=860, justify="left")
        current_label.pack(fill="x", padx=10)

        translate_label = tk.Label(root, text="è¯‘æ–‡ï¼š", font=("Microsoft YaHei", 16),
                                   fg="#ffff66", bg="black", anchor="w", wraplength=860, justify="left")
        translate_label.pack(fill="x", padx=10, pady=(0, 10))

        # å®æ—¶æ›´æ–°é€»è¾‘
        last_text = ""

        def update():
            nonlocal last_text
            try:
                while True:
                    item = SUBTITLE_QUEUE.get_nowait()
                    if isinstance(item, dict):
                        current_text = item.get("text", "")
                        audio_light.itemconfig("light_audio", fill=item.get("audio", "red"))
                        whisper_light.itemconfig("light_whisper", fill=item.get("whisper", "red"))
                        translate_light.itemconfig("light_translate", fill=item.get("translate", "red"))

                        if "åŸæ–‡:" in current_text:
                            parts = current_text.split("åŸæ–‡:")[1].split("\nç¿»è¯‘:")
                            original = parts[0].strip()
                            translated = parts[1].strip() if len(parts) > 1 else ""

                            # æ›´æ–°åº•éƒ¨å®æ—¶å­—å¹•
                            current_label.config(text=f"åŸæ–‡ï¼š{original}")
                            translate_label.config(text=f"è¯‘æ–‡ï¼š{translated}")

                            # åˆ¤æ–­æ˜¯å¦æ˜¯æ–°å¥å­ï¼Œè¿½åŠ åˆ°å†å²
                            if translated and current_text != last_text:
                                last_text = current_text
                                history_text.config(state="normal")
                                history_text.insert("end", f"åŸæ–‡ï¼š{original}\n", "origin")
                                history_text.insert("end", f"è¯‘æ–‡ï¼š{translated}\n\n", "trans")
                                history_text.see("end")
                                history_text.config(state="disabled")
                                history_text.tag_config("origin", foreground="#ccccff")
                                history_text.tag_config("trans", foreground="#ffff99")
            except queue.Empty:
                pass
            root.after(200, update)

        update()
        root.mainloop()

    threading.Thread(target=gui, daemon=True).start()


def start_ffmpeg():
    device_names = [
        'audio=CABLE Output (VB-Audio Virtual Cable)'
    ]
    for device in device_names:
        try:
            command = [
                'ffmpeg',
                '-f', 'dshow',
                '-i', device,
                '-ac', '1',
                '-ar', '16000',
                '-f', 'wav',
                'pipe:1'
            ]
            proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)

            def reader():
                while True:
                    header = proc.stdout.read(44)
                    data = proc.stdout.read(FRAME_SIZE * 10)
                    if not data:
                        break
                    AUDIO_QUEUE.put(data)

            threading.Thread(target=reader, daemon=True).start()
            print(f"ğŸ§ ä½¿ç”¨éŸ³é¢‘è¾“å…¥è®¾å¤‡: {device}")
            return
        except Exception as e:
            logging.warning(f"è®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {device} -> {e}")
            continue

    print("âŒ æœªèƒ½æˆåŠŸåˆå§‹åŒ–ä»»ä½•éŸ³é¢‘è®¾å¤‡")

def translate(text, max_retries=3):
    url = "https://api.siliconflow.cn/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "Qwen/Qwen3-14B",
        "messages": [
            {
                "role": "user",
                "content": f"è¯·å°†ä»¥ä¸‹æ–‡æœ¬å‡†ç¡®ç¿»è¯‘ä¸ºä¸­æ–‡ï¼š{text}"
            }
        ],
        "stream": False,
        "max_tokens": 512,
        "enable_thinking": False,
        "thinking_budget": 4096,
        "min_p": 0.05,
        "temperature": 0.7,
        "top_p": 0.7,
        "top_k": 50,
        "frequency_penalty": 0.5,
        "n": 1,
        "response_format": {"type": "text"},
        "tools": []
    }

    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=10)
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                logging.warning(f"ç¿»è¯‘è¯·æ±‚å¤±è´¥ï¼š{response.status_code} {response.text}")
        except requests.exceptions.RequestException as e:
            logging.error(f"[ç¿»è¯‘é‡è¯•] ç¬¬ {attempt+1} æ¬¡å¤±è´¥ï¼š{e}")
            time.sleep(1)

    # è‹¥ç¿»è¯‘å¤±è´¥åˆ™è¿”å›é”™è¯¯ä¿¡æ¯
    return "[ç¿»è¯‘å¤±è´¥]"


def wrap_text(text, max_length=20):
    # ç”¨äºè‡ªåŠ¨æ¢è¡Œçš„å‡½æ•°
    wrapped_text = ""
    while len(text) > max_length:
        wrapped_text += text[:max_length] + "\n"
        text = text[max_length:]
    wrapped_text += text  # æœ€åä¸€æ®µä¸æ¢è¡Œ
    return wrapped_text


def main_loop():
    last_text = ""
    while True:
        item = SUBTITLE_QUEUE.get()
        text = item.get("text", "")

        if item.get("need_translate"):
            # å¤„ç†åŸæ–‡æ–‡æœ¬
            pure = text.split("åŸæ–‡:")[1].strip().split("\n")[0]
            if pure.strip() and pure != last_text:
                last_text = pure
                translation = translate(pure)

                # å³ä¾¿ç¿»è¯‘å¤±è´¥ï¼Œä¹Ÿè¦ä¿ç•™åŸæ–‡
                item["text"] = f"åŸæ–‡: {pure}\nç¿»è¯‘: {translation if translation != '[ç¿»è¯‘å¤±è´¥]' else '[ç¿»è¯‘å¤±è´¥]'}"
                item["translate"] = "green" if translation.strip() and "å¤±è´¥" not in translation else "red"
                item["need_translate"] = False
                SUBTITLE_QUEUE.put(item)


start_gui()
start_ffmpeg()

print("ğŸ§ æ­£åœ¨ä½¿ç”¨é˜¿é‡Œäº‘å®æ—¶è¯­éŸ³è¯†åˆ«...")

recognizer = AliyunLiveRecognizer(
    token="",        # â† æ›¿æ¢ä¸ºä½ çš„å®æ—¶ Token
    appkey=""                        # â† æ›¿æ¢ä¸ºä½ çš„ AppKey
)

recognizer.start_stream()

main_loop()  # ç¿»è¯‘çº¿ç¨‹ï¼Œä¸»æ§å­—å¹•
